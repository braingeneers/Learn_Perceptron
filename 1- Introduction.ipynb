{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea2861ac",
   "metadata": {},
   "source": [
    "<font size=7>Introduction to Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329f199b",
   "metadata": {},
   "source": [
    "<font color=\"red\">   \n",
    "    \n",
    "**Haussler Questions**\n",
    "* is $u$ stationary?\n",
    "* is learning quantum computing worth it?\n",
    "* Summing positive negative weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7549c9bf",
   "metadata": {},
   "source": [
    "The perceptron is a machine learning algorithm that is used to make classifications between two different binary outcomes, $\\{-1,1\\}$. It has a rich history in neuroscience and AI. Here's a [video clip](https://www.youtube.com/watch?v=cNxadbrN_aI) of it from the 1950's! Here we provide an introduction to the perceptron algorithm. Later will be adapt the perceptron so that it closely resembles neurons in the brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f159a0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./Code/Introduction_Code.ipynb     # Load the code used in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8facc9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# <font color=\"gray\">Learn from Reading Material"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0828ea",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Though we describe the perceptron algorithm here, you should teach it to yourself using the reading material. Read the [wiki article](https://en.wikipedia.org/wiki/Perceptron) and chapter 5 of [Hertz's book](./Reading_Material/Hertz_Book_Chapter_5.pdf) on the perceptron. Also read the \"mathematical interlude\" in the Chapter 1 of [Susskind's book](./Reading_Material/Quantum_Chapter_1.pdf). The additional material below  provides a more in depth walkthrough of the perceptron algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7860fe3a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Required Reading Material**\n",
    "1. [Preceptron Wikipedia](https://en.wikipedia.org/wiki/Perceptron) - A good introduction\n",
    "2. [This video](https://www.youtube.com/watch?v=NogzKXE74AA) is a nice perceptorn overview. [This video](https://www.youtube.com/watch?v=4Gac5I64LM4) does basic perceptron exampls. Feel free to suggest other videos!\n",
    "4. [Theory of Neural Computation, Chapter 5 (Hertz)](./Reading_Material/Hertz_Book_Chapter_5.pdf) - Deep dive into perceptron. Also describes perceptron confidence threshold.\n",
    "3. [Pattern Recognition and Machine Learning, Chapter 3 (Bishop)](./Reading_Material/Bishop_Book_Chapter_3.pdf) - The perceptron is deeply described here. There's also a more general description of binary learning.\n",
    "4. [Quantum Mechanics, Chapter 1 (Susskind)](./Reading_Material/Quantum_Chapter_1.pdf) - Read the \"mathematical interlude\" section to learn abour bra-ket notation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e67ba2a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Advanced Material**\n",
    "\n",
    "1. [Perceptron Learning with Sign-Constrained Weights (Amit)](./Reading_Material/Signed_Perceptron_Paper_1.pdf)- Introduces framework for biorealistic perceptrons\n",
    "2. [Space of Interactions in Neural Network Models (Gardener)](./Reading_Material/Perceptron_Storage_Capacity.pdf)- Discusses the information storage capacity of perceptrons\n",
    "3. [Information Capacity of Perceptron vs Purkinje Cell (Brunel)](./Reading_Material/Perceptron_vs_Purkinje.pdf)- Compares properties of Perceptrons to the Purkinje Cells from which they were derived"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da780b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# What is the Perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4bf8e5",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Linear Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edb1f83",
   "metadata": {
    "cell_style": "center",
    "hidden": true
   },
   "source": [
    "Suppose you are given a a bucket of tulips and roses and are told to build a learning algorithm to classify the flowers' species based on their petals. For each flower you pluck a petal and measure it's width and length. You then plot the results. Run the code below to see the plot. What rule would you use to classify the flowers? An intuitive way to solve this problem is to draw a line on the plot that seperates tulips from roses. This is called a linear classifier. Use the interactive widgets below to fit a linear classifier to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "962df24f",
   "metadata": {
    "cell_style": "center",
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372a10bcf04b47488c303478e09a2c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='slope', max=2.0, min=-2.0, step=0.5), FloatSlider(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "guiLinearClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05680fb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's formalize what we've just done. For $n$ samples, define the\n",
    "petal measurements you took to be the inputs $\\mathbf{x}_{1},\\mathbf{x}_{2},...,\\mathbf{x}_{n}$\n",
    "and the flower species to be the target output $y_{1},y_{2},...,y_{n}$\n",
    ". In this example, there's two measurements so $\\vec{\\mathbf{x}}_{i}\\in\\mathbb{R}^{2}$\n",
    ". For binary classifiers we'll always have $y_{i}\\in\\{-1,1\\}$. We\n",
    "can define a linear classifier as the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46317099",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "f(\\mathbf{w},\\mathbf{x})=\\begin{cases}\n",
    "+1 & \\text{if }\\mathbf{w}\\cdot\\mathbf{x}-b>0\\\\\n",
    "-1 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a9c65c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The weights $\\vec{\\mathbf{w}}\\in\\mathbf{\\mathbb{R}}^{2}$ are parameters\n",
    "that determines the \"slope\" of the linear classifier. The bias,\n",
    "$b\\in\\mathbb{R}$, determines the intercept. To make $f(\\mathbf{w},\\mathbf{x})$\n",
    "look simpler, we can \"hide the bias\" in $\\mathbf{w}$ . Currently\n",
    "we have $\\mathbf{x}_{i}=[\\text{width},\\text{length}]$ , but instead\n",
    "let's write $\\mathbf{x}_{i}^{*}=[\\text{width},\\text{length},-1]$\n",
    ". For $\\mathbf{w}^{*}\\cdot\\mathbf{x}^{*}$ to still work we must then\n",
    "add an additional parameter to $\\mathbf{w}$, so $\\mathbf{w}^{*}=[w_{1},w_{2},w_{b}]\\in\\mathbf{\\mathbb{R}}^{3}$\n",
    ". If we set $b=w_{b}$ we have $\\mathbf{w}^{*}\\cdot\\mathbf{x}^{*}=\\mathbf{w}\\cdot\\mathbf{x}-b$\n",
    ". From now on we will always assume the bias is written into $\\mathbf{w},\\mathbf{x}$\n",
    "and disregard the notation $\\mathbf{w}^{*},\\mathbf{x}^{*}$. Thus\n",
    "we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cba9370",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "f(\\mathbf{w},\\mathbf{x})=\\begin{cases}\n",
    "+1 & \\text{if }\\mathbf{w}\\cdot\\mathbf{x}>0\\\\\n",
    "-1 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d0560",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab1a598",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How do we determine the values of $\\mathbf{w}$ that correctly classify\n",
    "the data? We use the perceptron algorithm! The perceptron algorithm\n",
    "works like this: Randomly select some initial weights, $\\mathbf{w}_{0}$\n",
    ", usually $||\\mathbf{w}_{0}||=1$ . One by one, cycle through each\n",
    "of the input/output pairs $\\mathbf{x}_{i},y_{i}$ . If the perceptron\n",
    "correctly classifies $y_{i}$ , which means $f(\\mathbf{w},\\mathbf{x}_{i})=y_{i}$\n",
    ", then do nothing. If it misclassifies $y_{i}$ , then update the\n",
    "weights according to the rule $\\mathbf{w}_{t+1}=\\mathbf{w}_{t}+y_{i}\\mathbf{x}_{i}$\n",
    ". Notice that after $\\mathbf{w}$ changes, the value of $f(\\mathbf{w},\\mathbf{x}_{i})$\n",
    "may also change. The algorithm keeps cycling through data until everything\n",
    "is classified correctly. Because it's important, let's state the perceptorn\n",
    "update rule again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbca924e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\mathbf{w}_{t+1}=\\begin{cases}\n",
    "\\mathbf{w}_{t}+y\\mathbf{x} & \\text{if }f(\\mathbf{w},\\mathbf{x})\\neq y\\\\\n",
    "\\mathbf{w}_{t} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb007571",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's restate the entire algorithm again using pseudocode. Let's call\n",
    "our data, $\\mathscr{D}=\\begin{bmatrix}(\\mathbf{x}_{1},y_{1}),(\\mathbf{x}_{2},y_{2}),...,(\\mathbf{x}_{n},y_{n})\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8147efd4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**<font size=4>Perceptron$(\\mathscr{D})$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4578f5d2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\mathbf{w}=$ Random values\n",
    "\n",
    "**WHILE:** $\\exists\\:f(\\mathbf{w},\\mathbf{x}_{i})\\neq y_{i}$ in $\\mathscr{D}$\n",
    "\n",
    "$\\quad$**FOR:** each $(\\mathbf{x}_{i},y_{i})\\in\\mathscr{D}$\n",
    "\n",
    "$\\quad$$\\quad$**IF:** $f(\\mathbf{w},\\mathbf{x}_{i})\\neq y_{i}$\n",
    "\n",
    "$\\quad$$\\quad$$\\quad$$\\mathbf{w}:=\\mathbf{w}+y\\mathbf{x}$\n",
    "\n",
    "**RETURN:** $\\mathbf{w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e264f10",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Perceptron Example <font color=\"red\">- Not Done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5d8d9",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918c9b70",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see the perceptron in action! \n",
    "\n",
    "\n",
    "<font color=\"gray\">To make this easy to visualize, we remove the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6189942",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd06f3493ef40e6a48e057a7f8780fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='updates', max=10), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "guiPerceptron()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2fd4e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Perceptron Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f64193",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This class is is over simplifid percetron code to get an idea of how it works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9133884f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, w ):\n",
    "        self.w = w                                               # Set initial weights when creating object\n",
    "\n",
    "    def update( self, x, y):                   \n",
    "        y_pred = -1. if np.dot(self.w, x)<0 else 1.              # Get models prediction for y        \n",
    "        self.w = self.w + (y!=y_pred) * float(y)*np.array(x)     # Update weights based on x,y,y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d14f391",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here is the data we used in the above example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e478ef53",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x=np.array([ [3.5,1.5], [-1.5,-2], [4,-1], [0,3.5], [-3.5,1], [-1.5,3], [2.5,2.5], [0.5,-4], [-3.5,-1.5], [-2.5,-2.5] ])\n",
    "y=np.array([-1,1,1,-1,-1,-1,-1,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58abb1f7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Notice that as we udpate the data, we get the exact same weights as show in the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e39c1f1d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update 1:\tw=[-1. -1.]\n",
      "Update 2:\tw=[-1. -1.]\n",
      "Update 3:\tw=[ 3. -2.]\n",
      "Update 4:\tw=[ 3. -2.]\n",
      "Update 5:\tw=[ 3. -2.]\n",
      "Update 6:\tw=[ 3. -2.]\n",
      "Update 7:\tw=[ 0.5 -4.5]\n",
      "Update 8:\tw=[ 0.5 -4.5]\n",
      "Update 9:\tw=[ 0.5 -4.5]\n",
      "Update 10:\tw=[ 0.5 -4.5]\n"
     ]
    }
   ],
   "source": [
    "learner = Perceptron([-1,-1])                  # Initial weights are w=[-1,-1]\n",
    "for i in range(len(y)):                        # Cycle through each datapoint\n",
    "    learner.update( x[i], y[i])                # Update the perceptron using x and y\n",
    "    print(f\"Update {i+1}:\\tw={learner.w}\")     # Print w for each update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df34bb6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# What is Dirac Notation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a58552c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\quad$In this course we will use [Dirac notation](https://en.wikipedia.org/wiki/Bra\\%E2\\%80\\%93ket\\_notation)\n",
    "(or Bra-Ket notation) created by the great physicist, Paul Dirac,\n",
    "to denote quantum states. Here we give a brief description. Read the\n",
    "\"mathematical interlude\" in chapter 1 of [Quantum Mechanics:\n",
    "The Theoretical Minimum](./Reading\\_Material/Quantum\\_Chapter\\_1.pdf)\n",
    "to learn more. This course requires learning many topics from quantum\n",
    "theory. Let's consider complex vectors that are written in a funny\n",
    "looking way $|\\mathbf{a}\\rangle,|\\mathbf{b}\\rangle\\in\\mathbb{C}^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5444ae4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{array}{ccc}\n",
    "|\\mathbf{a}\\rangle=\\begin{bmatrix}1\\\\\n",
    "i\n",
    "\\end{bmatrix} &  & |\\mathbf{b}\\rangle=\\begin{bmatrix}-1\\\\\n",
    "1+i\n",
    "\\end{bmatrix}\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885a6fe4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In Bra-Ket notation these vectors are called Ket vectors. The Ket\n",
    "vectors have corresponding Bra vectors, $\\langle\\mathbf{a}|=\\begin{bmatrix}1,-i\\end{bmatrix}$\n",
    "and $\\langle\\mathbf{b}|=\\begin{bmatrix}-1,1-i\\end{bmatrix}$ . In\n",
    "general, a Bra is the complex conjugate transpose of some Ket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60567ac",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\langle\\mathbf{a}|=|\\mathbf{a}\\rangle^{\\dagger}=\\overline{|\\mathbf{a}\\rangle}^{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a62b72",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We call the conjugate transpose the hermitian and symbolize it using\n",
    "$\\dagger$ . Let's now pretend $|\\mathbf{a}\\rangle$ , $|\\mathbf{b}\\rangle$\n",
    "only contain real numbers, then "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c842fb36",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\langle\\mathbf{a}|\\mathbf{b}\\rangle:=\\langle\\mathbf{a}||\\mathbf{b}\\rangle=|\\mathbf{a}\\rangle^{\\dagger}|\\mathbf{b}\\rangle\\equiv\\mathbf{a}{}^{T}\\mathbf{b}=\\mathbf{a}\\cdot\\mathbf{b}\\quad\\text{if }\\mathbf{a},\\mathbf{b}\\in\\mathbb{R}^{*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eb82cb",
   "metadata": {
    "hidden": true
   },
   "source": [
    " is the dot product after we switch back to traditional notation.\n",
    "Matrices and scalars can be squeezed in between Bra's and Ket's. $\\langle\\mathbf{a}|\\mathbf{C}y|\\mathbf{b}\\rangle\\equiv\\mathbf{a}^{\\dagger}\\mathbf{C}y\\mathbf{b}$\n",
    ". To give another example, when $\\mathbf{a}$ is real $\\langle\\mathbf{a}|\\mathbf{a}\\rangle\\equiv\\mathbf{a}\\cdot\\mathbf{a}=||\\mathbf{a}||^{2}$\n",
    ". We call $\\langle\\mathbf{a}|\\mathbf{a}\\rangle$ the inner product\n",
    "of $\\mathbf{a}$. Intuitively, the inner product is kind\n",
    "of like the magnitude. To give a Bra-Ket example with the Perceptron,\n",
    "we can rewrite the the linear classifier $f(\\mathbf{w},\\mathbf{x})$\n",
    "as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bc5aa1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "f(\\mathbf{w},\\mathbf{x})=\\begin{cases}\n",
    "+1 & \\text{if }\\langle\\mathbf{w}|\\mathbf{x}\\rangle>0\\\\\n",
    "-1 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb239fa",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Perceptron Convergence Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a967e8f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Any time a linear classifier can correclty classify all the data,\n",
    "we can show that the perceptron algorithm will find such a solution.\n",
    "Let's prove it! This proof works by considering how $\\mathbf{w}$\n",
    "changes as the number of updates to the algorithm $\\tau$ increases.\n",
    "Specifically, we will put upper and lower bounds on $\\langle\\mathbf{w}|\\mathbf{w}\\rangle$\n",
    "and $\\langle\\hat{\\mathbf{w}}|\\mathbf{w}\\rangle$ in terms of $\\tau$.\n",
    "We rely on the fact that the perceptron only stops when everything\n",
    "has been classified and then use the [Cauchy-Schwarz Inequality](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality)\n",
    "to show it must stop. Bishop's book, Chapter 3, contains a proof without\n",
    "Bra-Ket notation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2bc553",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Bound on $\\langle\\hat{\\mathbf{w}}|\\mathbf{w}\\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a21cf2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let start by finding a lower bound for $\\langle\\hat{\\mathbf{w}}|\\mathbf{w}\\rangle$.\n",
    "For a linear classifier to correctly classify all data, there must\n",
    "exist some weights $\\hat{\\mathbf{w}}$ for which $y_{i}=f(\\hat{\\mathbf{w}},\\mathbf{x}_{i})\\:\\forall i$\n",
    ". We start the perceptron algorithm with some weights $\\mathbf{w}_{0}$\n",
    "which, without loss of generality, are all zero. At each step of the\n",
    "algorithm, we change $\\mathbf{w}$ using the update rule "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482ae642",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "|\\mathbf{w}_{t+1}\\rangle=|\\mathbf{w}_{t}\\rangle+y_{i}|\\mathbf{x}_{i}\\rangle\\quad(1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c9cb6a",
   "metadata": {
    "cell_style": "center",
    "hidden": true
   },
   "source": [
    " where $|\\mathbf{x}_{i}\\rangle$ is a vector that is misclassified.\n",
    "After running the algorithm for a while, suppose that the number of\n",
    "times that each vector $|\\mathbf{x}_{i}\\rangle$ has been misclassified\n",
    "and updated is $\\tau_{i}\\in\\mathbb{N}$. Then the weights at this\n",
    "point are given by\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246490ad",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\\begin{array}{ccc}\n",
    "|\\mathbf{w}\\rangle=\\mathbf{w}_{0}+\\sum_{n}\\tau_{i}y_{i}|\\mathbf{x}_{i}\\rangle= & \\sum_{n}\\tau_{i}y_{i}|\\mathbf{x}_{i}\\rangle\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318a93a2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now take the inner product of the above equation with \\hat{\\mathbf{w}} to find our first bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd902b7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{array}{cl}\n",
    "\\langle\\hat{\\mathbf{w}}|\\mathbf{w}\\rangle & =\\sum_{n}\\langle\\hat{\\mathbf{w}}|\\tau_{i}y_{i}|\\mathbf{x}_{i}\\rangle\\\\\n",
    " & =\\sum_{n}\\tau_{i}y_{i}\\langle\\hat{\\mathbf{w}}|\\mathbf{x}_{i}\\rangle\\\\\n",
    " & \\geq\\tau\\min_{n}y_{i}\\langle\\hat{\\mathbf{w}}|\\mathbf{x}_{i}\\rangle\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d030e33e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Where $\\tau=\\sum_{n}\\tau_{i}$ is the total number of weight updates\n",
    "and the inequality results from replacing each update vector by the\n",
    "smallest possible update. We see that $\\langle\\hat{\\mathbf{w}}|\\mathbf{w}\\rangle$\n",
    "is bounded below by a function of $\\tau$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646d71fb",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Bound on $\\langle\\mathbf{w}|\\mathbf{w}\\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286d0c02",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lets now find an upper bound for $\\langle\\mathbf{w}|\\mathbf{w}\\rangle$.\n",
    "If we take the inner product of the update rule $(1)$ we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb526ef6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{array}{clc}\n",
    "\\langle\\mathbf{w}_{t+1}|\\mathbf{w}_{t+1}\\rangle & =\\langle\\mathbf{w}_{t}|\\mathbf{w}_{t}\\rangle+\\langle\\mathbf{x}_{i}|y_{i}^{2}|\\mathbf{x}_{i}\\rangle+2y_{i}\\langle\\mathbf{w}_{t}|\\mathbf{x}_{i}\\rangle\\\\\n",
    " & \\leq\\langle\\mathbf{w}_{t}|\\mathbf{w}_{t}\\rangle+\\langle\\mathbf{x}_{i}|y_{i}^{2}|\\mathbf{x}_{i}\\rangle\\\\\n",
    " & \\leq\\langle\\mathbf{w}_{t}|\\mathbf{w}_{t}\\rangle+\\langle\\mathbf{x}_{i}|\\mathbf{x}_{i}\\rangle & \\leftarrow\\text{since }y_{i}^{2}=1\\\\\n",
    " & \\leq\\langle\\mathbf{w}_{t}|\\mathbf{w}_{t}\\rangle+\\langle\\mathbf{x}|\\mathbf{x}\\rangle_{\\max}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3c7bf4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "where the first inequality follows from the fact that $|\\mathbf{x}_{i}\\rangle$\n",
    "was missclassified, look inside the equation $f(\\hat{\\mathbf{w}},\\mathbf{x})$\n",
    "to notice that this means $y_{i}\\langle\\mathbf{w}_{t}|\\mathbf{x}_{i}\\rangle<0$\n",
    ". We also define $\\langle\\mathbf{x}|\\mathbf{x}\\rangle_{\\max}$ as\n",
    "the inner product of the largest input so that $\\langle\\mathbf{x}_{i}|\\mathbf{x}_{i}\\rangle\\leq\\langle\\mathbf{x}|\\mathbf{x}\\rangle_{\\max}$\n",
    ". If we now consider the change $\\Delta$ in the value of $\\langle\\mathbf{w}|\\mathbf{w}\\rangle$\n",
    "we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244d3ef2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\Delta\\langle\\mathbf{w}|\\mathbf{w}\\rangle\\equiv\\langle\\mathbf{w}_{t+1}|\\mathbf{w}_{t+1}\\rangle-\\langle\\mathbf{w}_{t}|\\mathbf{w}_{t}\\rangle\\leq\\langle\\mathbf{x}|\\mathbf{x}\\rangle_{\\max}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5f266",
   "metadata": {
    "hidden": true
   },
   "source": [
    "and so after $\\tau$ updates we have the following upper bound on\n",
    "$\\langle\\mathbf{w}|\\mathbf{w}\\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63ad291",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\langle\\mathbf{w}|\\mathbf{w}\\rangle\\leq\\tau\\langle\\mathbf{x}|\\mathbf{x}\\rangle_{\\max}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349ee31f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Cauchy-Schwarz Inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e943b923",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The famous Cauchy-Schwarz Inequality states that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb14b4b7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\dfrac{|\\langle\\hat{\\mathbf{w}}|\\mathbf{w}\\rangle|^{2}}{\\langle\\mathbf{w}|\\mathbf{w}\\rangle\\langle\\hat{\\mathbf{w}}|\\hat{\\mathbf{w}}\\rangle}\\leq1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdb3644",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now show that as $\\tau$ grows large, the Cauchy Schwaurtz Inequality\n",
    "fails. Lets start by attempting to make the inequality work by plugging\n",
    "in the smallest posssible value of $\\langle\\hat{\\mathbf{w}}|\\mathbf{w}\\rangle$\n",
    "and the largest possible value of $\\langle\\mathbf{w}|\\mathbf{w}\\rangle$\n",
    "into the left side of the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea56a9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\\begin{array}{clc}\n",
    "\\dfrac{\\begin{pmatrix}\\tau\\min_{n}y_{i}\\langle\\hat{\\mathbf{w}}|\\mathbf{x}_{i}\\rangle\\end{pmatrix}^{2}}{\\tau\\langle\\mathbf{x}|\\mathbf{x}\\rangle_{\\max}\\langle\\hat{\\mathbf{w}}|\\hat{\\mathbf{w}}\\rangle} & =\\dfrac{\\tau^{2}\\begin{pmatrix}\\min_{n}y_{i}\\langle\\hat{\\mathbf{w}}|\\mathbf{x}_{i}\\rangle\\end{pmatrix}^{2}}{\\tau\\langle\\mathbf{x}|\\mathbf{x}\\rangle_{\\max}\\langle\\hat{\\mathbf{w}}|\\hat{\\mathbf{w}}\\rangle}\\\\\n",
    "\\\\\n",
    " & =\\tau\\dfrac{\\begin{pmatrix}\\min_{n}y_{i}\\langle\\hat{\\mathbf{w}}|\\mathbf{x}_{i}\\rangle\\end{pmatrix}^{2}}{\\langle\\mathbf{x}|\\mathbf{x}\\rangle_{\\max}\\langle\\hat{\\mathbf{w}}|\\hat{\\mathbf{w}}\\rangle}\\\\\n",
    "\\\\\n",
    " & =\\tau c & \\leftarrow c\\text{ is an abitrary positive number }\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401180fb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After we pull $\\tau$ out of the fraction the only important thing\n",
    "to know about the other values is that they're positive and not infinite\n",
    "(we start with finite data). As we run our algorithm, the number of\n",
    "steps $\\tau$ grows. We cannot run our algorithm forever, because\n",
    "then $\\tau c=\\infty>1$ breaking Cauchy-Schwartz's Inequality. The\n",
    "perceptron algorithm must eventually stop, which only occures when\n",
    "all the data is is correctly classified. Thus we have proven the perceptron\n",
    "convergence theorem."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
